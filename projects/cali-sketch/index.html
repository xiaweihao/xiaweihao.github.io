<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Stroke Calibration and Completion for High-Quality Face Image Generation</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Video-Inpainting.&gt;
&lt;meta name=" keywords"="">

<!-- Fonts and stuff -->
<link rel="stylesheet" type="text/css" href="../project.css">
<link rel="stylesheet" type="text/css" href="../project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="../iconize.css">
<script type="text/javascript" async="" src="../ga.js.download"></script><script async="" src="../prettify.js.download"></script>

<script type="text/javascript">
            
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-22940424-1']);
            _gaq.push(['_trackPageview']);
            
            (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
            })();
            
</script>

</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
    <h1>Stroke Calibration and Completion for High-Quality Face Image Generation</h1>

    <div class="authors">
      <a href="https://xiaweihao.github.io/">Weihao Xia<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a>Yujiu Yang<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://www.homepages.ucl.ac.uk/~ucakjxu/">Jing-Hao Xue<sup>2</sup></a>
    </div>

    <div class="affiliations">
      <sup>1</sup> <a>Tsinghua University</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>
      <sup>2</sup> <a>Department of Statistical Science, University College London</a>
    </div>

    <div class="venue"></div>
  </div>


  <div class="section abstract">
    <h2>Abstract</h2>
    <br>
    <p>
        Image-to-image translation aims to translate an image of one domain to a given reference image of another domain. When applied to specific tasks, e.g, given edges and target images that follow strict alignment, supervised image-to-image translation methods can produce good translation results. However, when it comes to a poorly drawn sketch created by non-artists, the images generated by these methods are unacceptable. Furthermore, these free-hand sketches are still expressive in conveying facial features or emotion, and existing sketch-to-image generation methods fail to preserve this kind of information. In this paper, we propose a badly-drawn-sketch to face image generation method, named Cali-Sketch. It explicitly models stroke calibration and image generation using two components: Stroke Calibration Network (SCN), which calibrates strokes of facial features and enriches facial details while preserving the original intent features, and Image Synthesis Network (ISN), which translates the modified sketches after calibration and completion to face photos. Thus we decouple a difficult cross-domain translation problem into two easier steps. Extensive experiments show that face photos generated by our method are both photo-realistic and faithful to the input sketches, compared to state-of-the-art methods.
    </p>
  </div>

<div class="section materials">
  <h2>Materials</h2>
  <center>
    <ul>
          <li class="grid">
        <div class="griditem">
    <a href="./docs/Cali-Sketch.pdf" target="_blank" class="imageLink"><img src="./pic/paper.png" border="0" width="50%"></a><br>
    </div>
        </li>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      
      <li class="grid">
        <div class="griditem">
    <a href="https://github.com/nbei/Deep-Flow-Guided-Video-Inpainting"><img src="./pic/code.png"></a><br>
    </div>
        </li>
    
      </ul>
      </center>
      </div>


    <div class="section framework">
    <h2>Framework</h2>
    <br>
        <center><img src="./pic/framework.png" border="0" width="95%"></center>
    <br>
    <p>
          We decompose this translation into two stages: 
          1) Stroke Calibration Network named SCN, and 2) Image Synthesis Network named ISN. Let G1 and D1 be the generator and discriminator of SCN, G2 and D2 be the generator and discriminator of ISN, respectively.
          As shown in the Framework, the input sketch S is first put into SCN to get the refined sketch R after stroke calibration and detail completion, which is then fed into ISN to generate a photo-realistic face image P. We first train Stroke Calibration Network and Image Synthesis Network separately until the losses plateau, and then train them jointly in an end-to-end way until convergence.
    </p>
    </div>

        <div class="section visualization">
            <h2>Visualization</h2>
<!--             <p>
            Illustration of four different free-hand sketch styles: photo-sketch, XDoG, Photocopy and FDoG. To exhibit different styles of free-hand sketches and to improve the network generality, we augment training data by adopting multiple different styles of input sketches. Specifically, we generate four different free-hand sketch styles in total. We use the XDoG edge detector , Photocopy effect in Photoshop to generate two styles. To better resemble hand-drawn sketches, we simplified the edge images using FDoG. We also use photo-sketch to generate the desired face sketches. This recent method generates imperfect alignment contour sketches of input images. The badly-drawn sketches should be sparse and contain wrong edges. That’s why the Canny algorithm shouldn’t be chosen to get input sketches. Those edges generated by Canny are solid and well-aligned with input images. To show effectiveness and efficiency of our approach, the CUHK Face Sketch Database is used in our experiment for its appropriateness and popularity.
            </p>
            <br>
                <center><img src="./projects/cali-sketch/sketch_style.png" border="0" width="50%"></center>
            <p>
                <center>Illustration of four different free-hand sketch styles: photo-sketch, XDoG, Photocopy and FDoG.</center>
            </p> -->
            <p>
             Qualitative comparison with baselines. We compare our methods with pix2pix [1], CycleGAN [2], DRIT [3], MUNIT [4]. Our approach generates high-uality images. The generated human face images are more photo-realistic. The corresponding image can be recognised easily from a batch of mixed sketches, which means crucial components and drawing intention of original sketches like facial contours, hair styles are well-preserved in the synthesized images.<b> (Best viewed in with zoom-in.)</b>
            </p>
            
            <center><img src="./pic/visual-compare.png" border="0" width="80%"></center>
            <p>
            <center>Comparision with state-of-the-art methods.</center>
            </p>
        </div>

        
<br>

<div class="section citation">
    <h2>Citation</h2>
    <div class="section bibtex">
      <pre>@InProceedings{Xia_2019_Sketch,
author = {Xia, Weihao and Yang, Yujiu and Xue, Jing-Hao},
title = {Stroke Calibration and Completion for High-Quality Face Image Generation},
<!-- booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, -->
month = {April},
year = {2019}
}</pre>
      </div>
      </div>

<div class="section materials">
    <h2>References</h2>
      <ol>
        <li>Jun-Yan Zhu*, Taesung Park*, Phillip Isola, Alexei A. Efros <a>"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks".</a> ICCV, 2017.</li>
        <li>Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman <a>"Toward Multimodal Image-to-Image Translation".</a> NIPS, 2017.</li>
        <li>Hsin-Ying Lee*, Hung-Yu Tseng*, Jia-Bin Huang, Maneesh Kumar Singh, Ming-Hsuan Yang, <a>"Diverse Image-to-Image Translation via Disentangled Representations".</a> ECCV, 2018.</li>
        <li>Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz <a>"MUNIT: Multimodal UNsupervised Image-to-image Translation".</a> ECCV, 2018.</li>

      </ol>
      </div>




</div></div></body></html>