<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Domain-Aware No-Reference Image Quality Assessment</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Video-Inpainting.&gt;
&lt;meta name=" keywords"="">

<!-- Fonts and stuff -->
<link rel="stylesheet" type="text/css" href="../project.css">
<link rel="stylesheet" type="text/css" href="./css/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="../iconize.css">
<script type="text/javascript" async="" src="../ga.js.download"></script>
<script async="" src="../prettify.js.download"></script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$']]},
        messageStyle: "none"
    });
</script>

<script type="text/javascript">
            
            var _gaq = _gaq || [];
            _gaq.push(['_setAccount', 'UA-22940424-1']);
            _gaq.push(['_trackPageview']);
            
            (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
            })();
            
</script>

</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
    <h1>Controllable Continuous Gaze Redirection</h1>

    <div class="authors">
      <a href="https://xiaweihao.github.io/">Weihao Xia<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a>Yujiu Yang<sup>1</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://www.homepages.ucl.ac.uk/~ucakjxu/">Jing-Hao Xue<sup>2</sup></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a>Wensen Feng<sup>3</sup></a>
    </div>

    <div class="affiliations">
<!--       <sup>1</sup> <a>Tsinghua University</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>
      <sup>2</sup> <a>Department of Statistical Science, University College London</a> -->
      <sup>1</sup> <a>Tsinghua University</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <sup>2</sup> <a>UCL</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <sup>3</sup> <a>Beijing University of Chemical Technology</a>
    </div>

    <div class="venue"></div>
  </div>
  

  <div class="section abstract">
    <h2>Abstract</h2>
     <br>
        <center><img src="./pic/teaserfigure.jpg" border="0" width="95%"></center>
    <br>
    <br>
    <p>
      In this work, we present interpGaze, a novel framework for controllable gaze redirection that achieves both precise redirection and continuous interpolation. Given two gaze images with different attributes, our goal is to redirect the eye gaze of one person into any gaze direction depicted in the reference image or to generate continuous intermediate results. To accomplish this, we design a model including three cooperative components: an encoder, a controller and a decoder. The encoder maps images into a well-disentangled and hierarchically-organized latent space. The controller adjusts the magnitudes of latent vectors to the desired strength of corresponding attributes by altering a control vector. The decoder converts the desired representations from the attribute space to the image space. To facilitate covering the full space of gaze directions, we introduce a high-quality gaze image dataset with a large range of directions, which also benefits researchers in related areas. Extensive experimental validation and comparisons to several baseline methods show that the proposed interpGaze outperforms state-of-the-art methods in terms of image quality and redirection precision.
    </p>
      </div>
    
    <div class="section materials">
  <h2>Materials</h2>
  <center>
    <ul>
          <li class="grid">
        <div class="griditem">
    <a href="https://arxiv.org/abs/2010.04513" target="_blank" class="imageLink"><img src="./pic/paper.png" border="0" width="50%"></a><br> arxiv
    </div>
        </li>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      
      <li class="grid">
      <div class="griditem">
        <a href="https://github.com/weihaox/InterpGaze"><img src="./pic/code.png"></a><br> github (coming)
     </div>
        </li>
    
      </ul>
      </center>
      </div>

    <div class="section framework">
    <h2>Framework</h2>
    <br>
        <center><img src="./pic/overview.jpg" border="0" width="95%"></center>
    <br>
   <p>
      Our proposed model contains (a) an <I>Encoder</I> $\boldsymbol{{E}}$, (b) a <I>Controller</I> $\boldsymbol{\mathcal{C}}$ and (c) a <I>Decoder</I> $\boldsymbol{G}$. The Encoder $\boldsymbol{E}$ maps images $\boldsymbol{x}_s$ and $\boldsymbol{x}_t$ into feature space $F_{s}=\boldsymbol{E}\left(x_{s}\right)$ and  $F_{t}=\boldsymbol{E}\left(x_{t}\right)$. Then the feature difference is fed into four branches of the controller $\boldsymbol{\mathcal C}$ to produce morphing results of two samples $\mathcal {C}_{\boldsymbol{v}}(F_{s},F_{t}) =F_{s}+\sum_{k=1}^{c+1}{\boldsymbol{v}}^{k} \mathcal{T}^{k}(F_{t}-F_{s})$. The abbreviations P, H, V and O are head pose (P), vertical gaze direction (pitch, V), horizontal gaze direction (yaw, H) and miscellaneous attributes. The ``O'' branch is designed for other secondary attributes like glass, eyebrow, skin color, hair and illumination. The control vector $\boldsymbol{v} \in[0,1]^{(c+1) \times 1}$ adjusts the strength of each attribute, where $c=3$ in current setting. The Decoder $\boldsymbol{G}$ maps the latent features back to the image space.
    </p>
    </div>

    <div class="section visualization">
        <h2>Results</h2><br>
        <p>
        	This picture is illustration of interpolation between two given samples (green and blue). It can be seen that other attributes like eyebrow, glass, hair and skin color are well-preserved in the redirected gaze images, which means our model works consistently well in generating person-specific gaze images. Furthermore, since the encoder actually unfolds the natural image manifold, leading to a flat and smooth latent space that allows interpolation and even extrapolation, as shown in the last column.
        </p>
        <br>
            <center><img src="./pic/interpolation.jpg" border="0" width="80%"></center>
        <br>
    </div>
<br>

<div class="section citation">
<h2>Citation</h2>
<div class="section bibtex">
<p>If you find our work, code or pre-trained models helpful for your research, please consider to cite:</p>
<pre>@inproceedings{xia2020gaze,
  title={Controllable Continuous Gaze Redirection},
  author={Xia, Weihao and Yang, Yujiu and Xue, Jing-Hao and Feng, Wensen},
  year={2020},
  booktitle={ACM MM},
}</pre>
</div>
</div>



</div></div></body></html>